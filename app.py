import streamlit as st
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_community.llms import Ollama

# -------------------------------
# ğŸŒŸ PAGE CONFIG
# -------------------------------
st.set_page_config(page_title="AI Policy Compliance Assistant", page_icon="ğŸ¤–", layout="wide")

st.title("ğŸ¤– AI Policy Compliance Assistant")
st.write("Ask questions about GDPR, India's DPDP Act, and AI Ethics â€” all running locally with Ollama.")

# -------------------------------
# âš™ï¸ LOAD SYSTEM COMPONENTS
# -------------------------------
@st.cache_resource
def load_system():
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    db = Chroma(persist_directory="chroma_store", embedding_function=embeddings)
    retriever = db.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 10, "fetch_k": 30, "lambda_mult": 0.7}
    )
    # ğŸ‘‡ Choose a model that fits your memory
    llm = Ollama(model="phi3:mini", temperature=0.3)  # tinyllama / gemma:2b / mistral
    qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
    return qa, retriever

qa, retriever = load_system()

# -------------------------------
# ğŸ§© SIDEBAR
# -------------------------------
st.sidebar.header("ğŸ“„ Documents Loaded:")
st.sidebar.markdown("""
- ğŸ‡ªğŸ‡º GDPR (EU Data Protection)
- ğŸ‡®ğŸ‡³ India DPDP Act (2023)
- ğŸŒ UNESCO AI Ethics Guidelines
""")
st.sidebar.info("All data is processed locally â€” no internet or API keys required âœ…")

# -------------------------------
# ğŸ’¬ QUESTION INPUT
# -------------------------------
st.markdown("---")
st.subheader("ğŸ’¬ Ask a Question or Compare Laws")

question = st.text_area(
    "Enter your question below:",
    placeholder="e.g., Compare GDPR and India's DPDP Act on user consent and data retention.",
    height=100
)

# -------------------------------
# ğŸš€ ANALYSIS LOGIC
# -------------------------------
if st.button("Generate Answer ğŸš€"):
    if question.strip() == "":
        st.warning("Please enter a question first.")
    else:
        with st.spinner("Retrieving relevant context..."):
            docs = retriever.get_relevant_documents(question)

        sources = {d.metadata.get("source", "unknown") for d in docs}

        # Show retrieved context
        with st.expander("ğŸ” Retrieved Context (Top 5 Chunks)"):
            for i, d in enumerate(docs[:5]):
                st.markdown(f"**Chunk {i+1} â€” from _{d.metadata.get('source', 'unknown')}_**")
                st.write(d.page_content[:600] + "...")
                st.markdown("---")

        # Build structured comparison prompt
        context_text = "\n\n".join(
            [f"From {d.metadata.get('source', 'unknown')}:\n{d.page_content[:800]}" for d in docs[:6]]
        )

        compare_prompt = f"""
You are a senior legal policy analyst specializing in data protection and AI ethics.
Use only the provided context from GDPR, DPDP, and AI Ethics guidelines to answer the user's question.

Present the answer in a structured, professional format using Markdown headings and bullet points.

ğŸ“š Context from documents:
{context_text}

â“ Question:
{question}

ğŸ§¾ Output Format:
### ğŸ§­ Overview
### ğŸ‡ªğŸ‡º GDPR Highlights
### ğŸ‡®ğŸ‡³ DPDP Act Highlights
### âš–ï¸ Key Similarities
### ğŸš© Key Differences
### ğŸ§© Summary
"""

        with st.spinner("ğŸ¤– Generating structured answer..."):
            answer = qa.run(compare_prompt)

        st.subheader("ğŸ“Š Final Answer")
        st.markdown(answer)
        st.markdown(f"ğŸ“š **Sources used:** {', '.join(list(sources))}")
        st.success("âœ… Done!")